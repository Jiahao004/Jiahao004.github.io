---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


* I am interning for Long Reasoning Models in the [Tencent AI Lab](https://ailab.tencent.com/ailab/zh/index). 
* I am also a Ph.D. candidate at [NTU](https://www.ntu.edu.sg/) supervised by [Prof. Chen Lihui](https://scholar.google.com/citations?user=XqNeXssAAAAJ&hl=en).
* I obtained my MSc Degree in Signal Processing at the same university. Before that, I obtained my Bachelor's degree at [UESTC](https://en.uestc.edu.cn/).

Projects
---
- **Scaling RL-Zero**: Towards more general and frontier reasoning tasks.
- **TransAgents**: A virtual multi-agent translation company that mirrors the traditional translation publication processes.[Paper](https://aclanthology.org/2024.emnlp-demo.14/).[Code](https://github.com/minghao-wu/transagents).
- **Complex instruction following**: Project for Hunyuan LLMs.
- **Seamless speech-to-speech translation**: Speach-to-speech translation model for Tencent Meeting App.
- **Privacy-preserving NMT system**: A NMT translation system, which auto-protects the privacy information.
- **Back Translation NMT system**: A back translation data algorithm to enhance the performance of the Tencent Translator App.


Selected Publications
---
\* denotes co-first authors.
### LLM Reasoning
* Li, Y.\*, <span style="color:red">Xu, J.\*</span>, Liang, T.\*, Chen, X., He, Z., Liu, Q., Wang, R., Zhang, Z., Tu, Z., Mi, H., and Yu, D. [Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique](http://dx.doi.org/10.13140/RG.2.2.27912.33289).
* Ji, K.\*, <span style="color:red">Xu, J.\*</span>, Liang, T.\*, Liu, Q.\*, He, Z., Chen, X., Liu, X., Wang, Z., Chen, J., Wang, B. and Tu, Z., [The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models](http://dx.doi.org/10.13140/RG.2.2.33772.07043). 
* Wang, Y.\*, Liu, Q.\*, <span style="color:red">Xu, J.\*</span>, Liang, T.\*, Chen, X.*, He, Z.*, Song, L., Yu, D., Li, J., Zhang, Z. and Wang, R., 2025. [Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs](https://arxiv.org/abs/2501.18585). arXiv preprint arXiv:2501.18585.  
* Chen, X.\*, <span style="color:red">Xu, J.\*</span>, Liang, T.\*, He, Z.\*, Pang, J., Yu, D., Song, L., Liu, Q., Zhou, M., Zhang, Z. and Wang, R., 2024. [Do not think that much for 2+ 3=? on the overthinking of o1-like llms](https://arxiv.org/abs/2412.21187). ICML2025
* Yu, D., Zhang, Y., <span style="color:red">Xu, J.</span>, Liang, T., Song, L., Tu, Z., Mi, H. and Yu, D., 2024. [Teaching LLMs to Refine with Tools](https://arxiv.org/abs/2412.16871). arXiv preprint arXiv:2412.16871  
* Lin, Z.\*, Liang, T.\*, <span style="color:red">Xu, J.\*</span>, Wang, X., Luo, R., Shi, C., Li, S., Yang, Y. and Tu, Z., 2024. [Critical Tokens Matter: Token-Level Contrastive Estimation Enhence LLM's Reasoning Capability](https://arxiv.org/abs/2411.19943). ICML2025

### Inference Acceleration
* Zhang, Z., <span style="color:red">Xu, J.</span>, Liang, T., Chen, X., He, Z., Wang, R. and Tu, Z., 2024. [Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding](https://arxiv.org/abs/2411.18462). arXiv preprint arXiv:2411.18462.  


### Semantic Sentence Embeddings
* <span style="color:red">Xu, J.</span>, Zhanyi, C.S., Xu, L. and Chen, L., 2024. [Blendcse: Blend contrastive learnings for sentence embeddings with rich semantics and transferability](https://www.sciencedirect.com/science/article/abs/pii/S0957417423024119). Expert Systems with Applications, 238, p.121909.  
* <span style="color:red">Xu, J.</span>, Shao, W., Chen, L. and Liu, L., 2023, December. [DistillCSE: Distilled Contrastive Learning for Sentence Embeddings](https://aclanthology.org/2023.findings-emnlp.547/). In Findings of the Association for Computational Linguistics: EMNLP 2023 (pp. 8153-8165).  
* <span style="color:red">Xu, J.</span>, Shao, W., Chen, L. and Liu, L., 2023, December. [SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives](https://aclanthology.org/2023.emnlp-main.737/). In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (pp. 12028-12040).  


### Neural Machine Translation
* <span style="color:red">Xu, J.</span>, Ruan, Y., Bi, W., Huang, G., Shi, S., Chen, L. and Liu, L., 2022, July. [On Synthetic Data for Back Translation](https://aclanthology.org/2022.naacl-main.32/). In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 419-430).



Awards
---
* 2024 Technical Breakthrough Award from Tencent Hunyuan Large Model Joint Team
* NTU Research Scholarship


Acknowledgment
---
- I would like to express my heartfelt gratitude to Principal Researcher [Zhaopeng Tu](https://tuzhaopeng.github.io/) and [Lemao Liu](https://lemaoliu.github.io/) for their invaluable guidance and support during my internship at Tencent AI Lab. Their expertise, encouragement, and insightful feedback have been instrumental in shaping my understanding of long-reasoning models and advancing my research skills.  
- I would also like to extend my thanks to the interns who worked alongside me during this journey. Their collaboration, diverse perspectives, and shared enthusiasm for innovation greatly enriched my experience and made our work environment both dynamic and inspiring.


<a href="https://mapmyvisitors.com/web/1byac"  title="Visit tracker"><img src="https://mapmyvisitors.com/map.png?d=LOcnqrZfCnctscm__hrpxb47aA9TF9wgOBIzfhgJnPg&cl=ffffff" /></a>
